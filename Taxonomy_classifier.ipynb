{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cQhXv95r0mjB2otb91sdNSihPXZHWnPJ",
      "authorship_tag": "ABX9TyNN1j/uIcQ1nRydJEeoJ7l2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/klarkapet/Project_Winter22/blob/main/Taxonomy_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "p2HAAp6RwPNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12854380-ffd6-4fbe-98eb-5a7db8d0a986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/anuradha1992/EmpatheticIntents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6cguNkbSOkH",
        "outputId": "7ce48949-1e15-4023-ca45-7d21cefe8ed0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'EmpatheticIntents' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# git remote add origin https://{username}:{password}@github.com/{username}/project.git\n",
        "! git init\n",
        "! git config — global user.email “klarka.petrovicka@gmail.com”\n",
        "! git config — global user.name “klarkapet” \n",
        "# clone https://github.com/klarkapet/Project_Chatbot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5nbTcgdH92C",
        "outputId": "0482b674-8b1a-45c4-f35e-7df25eaf366f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized empty Git repository in /content/.git/\n",
            "usage: git config [<options>]\n",
            "\n",
            "Config file location\n",
            "    --global              use global config file\n",
            "    --system              use system config file\n",
            "    --local               use repository config file\n",
            "    -f, --file <file>     use given config file\n",
            "    --blob <blob-id>      read config from given blob object\n",
            "\n",
            "Action\n",
            "    --get                 get value: name [value-regex]\n",
            "    --get-all             get all values: key [value-regex]\n",
            "    --get-regexp          get values for regexp: name-regex [value-regex]\n",
            "    --get-urlmatch        get value specific for the URL: section[.var] URL\n",
            "    --replace-all         replace all matching variables: name value [value_regex]\n",
            "    --add                 add a new variable: name value\n",
            "    --unset               remove a variable: name [value-regex]\n",
            "    --unset-all           remove all matches: name [value-regex]\n",
            "    --rename-section      rename section: old-name new-name\n",
            "    --remove-section      remove a section: name\n",
            "    -l, --list            list all\n",
            "    -e, --edit            open an editor\n",
            "    --get-color           find the color configured: slot [default]\n",
            "    --get-colorbool       find the color setting: slot [stdout-is-tty]\n",
            "\n",
            "Type\n",
            "    --bool                value is \"true\" or \"false\"\n",
            "    --int                 value is decimal number\n",
            "    --bool-or-int         value is --bool or --int\n",
            "    --path                value is a path (file or directory name)\n",
            "    --expiry-date         value is an expiry date\n",
            "\n",
            "Other\n",
            "    -z, --null            terminate values with NUL byte\n",
            "    --name-only           show variable names only\n",
            "    --includes            respect include directives on lookup\n",
            "    --show-origin         show origin of config (file, standard input, blob, command line)\n",
            "\n",
            "usage: git config [<options>]\n",
            "\n",
            "Config file location\n",
            "    --global              use global config file\n",
            "    --system              use system config file\n",
            "    --local               use repository config file\n",
            "    -f, --file <file>     use given config file\n",
            "    --blob <blob-id>      read config from given blob object\n",
            "\n",
            "Action\n",
            "    --get                 get value: name [value-regex]\n",
            "    --get-all             get all values: key [value-regex]\n",
            "    --get-regexp          get values for regexp: name-regex [value-regex]\n",
            "    --get-urlmatch        get value specific for the URL: section[.var] URL\n",
            "    --replace-all         replace all matching variables: name value [value_regex]\n",
            "    --add                 add a new variable: name value\n",
            "    --unset               remove a variable: name [value-regex]\n",
            "    --unset-all           remove all matches: name [value-regex]\n",
            "    --rename-section      rename section: old-name new-name\n",
            "    --remove-section      remove a section: name\n",
            "    -l, --list            list all\n",
            "    -e, --edit            open an editor\n",
            "    --get-color           find the color configured: slot [default]\n",
            "    --get-colorbool       find the color setting: slot [stdout-is-tty]\n",
            "\n",
            "Type\n",
            "    --bool                value is \"true\" or \"false\"\n",
            "    --int                 value is decimal number\n",
            "    --bool-or-int         value is --bool or --int\n",
            "    --path                value is a path (file or directory name)\n",
            "    --expiry-date         value is an expiry date\n",
            "\n",
            "Other\n",
            "    -z, --null            terminate values with NUL byte\n",
            "    --name-only           show variable names only\n",
            "    --includes            respect include directives on lookup\n",
            "    --show-origin         show origin of config (file, standard input, blob, command line)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pytorch_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpudIn4VUbaA",
        "outputId": "8bce270c-d3fc-4fbe-f85b-9930b455b795"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_transformers\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[K     |████████████████████████████████| 176 kB 26.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (4.64.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 60.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.21.6)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (2022.6.2)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.12.1+cu113)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.4-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 61.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch_transformers) (4.1.1)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 681 kB/s \n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.4\n",
            "  Downloading botocore-1.29.4-py3-none-any.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 56.8 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 66.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.4->boto3->pytorch_transformers) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.4->boto3->pytorch_transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 71.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (2022.9.24)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (1.2.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=012fa8490ec35de21138238f1214d49c444598f50e60ee2ad1cadf71e7ce721b\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, sacremoses, boto3, pytorch-transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.26.4 botocore-1.29.4 jmespath-1.0.1 pytorch-transformers-1.2.0 s3transfer-0.6.0 sacremoses-0.0.53 sentencepiece-0.1.97 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from EmpatheticIntents import model, utilities, optimize, create_datasets\n",
        "from pytorch_transformers import RobertaTokenizer\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import csv"
      ],
      "metadata": {
        "id": "vURqWz-BSihz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tf.compat.v1.enable_eager_execution()"
      ],
      "metadata": {
        "id": "6q7W9SmhNKF4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "num_layers = 12\n",
        "d_model = 768\n",
        "num_heads = 12\n",
        "dff = d_model * 4\n",
        "hidden_act = 'gelu'  # Use 'gelu' or 'relu'\n",
        "dropout_rate = 0.1\n",
        "layer_norm_eps = 1e-5\n",
        "max_position_embed = 514\n",
        "num_emotions = 41  # Number of emotion + intent categories\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "vocab_size = tokenizer.vocab_size\n",
        "max_length = 100  # Maximum number of tokens\n",
        "buffer_size = 100000\n",
        "batch_size = 32\n",
        "# num_epochs = 10\n",
        "peak_lr = 2e-5\n",
        "total_steps = 7000\n",
        "warmup_steps = 700\n",
        "adam_beta_1 = 0.9\n",
        "adam_beta_2 = 0.98\n",
        "adam_epsilon = 1e-6"
      ],
      "metadata": {
        "id": "2iX8zVT_TzvQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f69b4b0b-d29d-45c9-fc96-af34a2821e84"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 898823/898823 [00:01<00:00, 639459.97B/s]\n",
            "100%|██████████| 456318/456318 [00:00<00:00, 459230.08B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emotions = ['afraid',\n",
        "            'angry',\n",
        "            'annoyed',\n",
        "            'anticipating',\n",
        "            'anxious',\n",
        "            'apprehensive',\n",
        "            'ashamed',\n",
        "            'caring',\n",
        "            'confident',\n",
        "            'content',\n",
        "            'devastated',\n",
        "            'disappointed',\n",
        "            'disgusted',\n",
        "            'embarrassed',\n",
        "            'excited',\n",
        "            'faithful',\n",
        "            'furious',\n",
        "            'grateful',\n",
        "            'guilty',\n",
        "            'hopeful',\n",
        "            'impressed',\n",
        "            'jealous',\n",
        "            'joyful',\n",
        "            'lonely',\n",
        "            'nostalgic',\n",
        "            'prepared',\n",
        "            'proud',\n",
        "            'sad',\n",
        "            'sentimental',\n",
        "            'surprised',\n",
        "            'terrified',\n",
        "            'trusting']\n",
        "\n",
        "labels = ['afraid', \n",
        "            'angry',\n",
        "            'annoyed',\n",
        "            'anticipating',\n",
        "            'anxious',\n",
        "            'apprehensive',\n",
        "            'ashamed',\n",
        "            'caring',\n",
        "            'confident',\n",
        "            'content',\n",
        "            'devastated',\n",
        "            'disappointed',\n",
        "            'disgusted',\n",
        "            'embarrassed',\n",
        "            'excited',\n",
        "            'faithful',\n",
        "            'furious',\n",
        "            'grateful',\n",
        "            'guilty',\n",
        "            'hopeful',\n",
        "            'impressed',\n",
        "            'jealous',\n",
        "            'joyful',\n",
        "            'lonely',\n",
        "            'nostalgic',\n",
        "            'prepared',\n",
        "            'proud',\n",
        "            'sad',\n",
        "            'sentimental',\n",
        "            'surprised',\n",
        "            'terrified',\n",
        "            'trusting',\n",
        "            'agreeing',\n",
        "            'acknowledging',\n",
        "            'encouraging',\n",
        "            'consoling',\n",
        "            'sympathizing',\n",
        "            'suggesting',\n",
        "            'questioning',\n",
        "            'wishing',\n",
        "            'neutral']"
      ],
      "metadata": {
        "id": "LRh6sxB9NDRO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights_path = '/content/gdrive/MyDrive/Colab Notebooks/Checkpoints_empathy_taxonomy/pretrained_weights/roberta2emobert.h5'\n",
        "checkpoint_path = '/content/gdrive/MyDrive/Colab Notebooks/Checkpoints_empathy_taxonomy/checkpoints'\n",
        "data_path = '/content/EmpatheticIntents/datasets/train_data'"
      ],
      "metadata": {
        "id": "eWI6CGkEIju-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "tax_model = model.EmoBERT(num_layers, d_model, num_heads, dff, hidden_act, dropout_rate, layer_norm_eps, max_position_embed, vocab_size, num_emotions)\n",
        "utilities.build_model(tax_model, max_length, vocab_size)\n",
        "tax_model.load_weights(weights_path)"
      ],
      "metadata": {
        "id": "Tna0-9FMSvJj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer and metrics\n",
        "learning_rate = optimize.CustomSchedule(peak_lr, total_steps, warmup_steps)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1 = adam_beta_1, beta_2 = adam_beta_2,\n",
        "  epsilon = adam_epsilon)\n",
        "\n",
        "#train_loss = tf.keras.metrics.Mean(name = 'train_loss')"
      ],
      "metadata": {
        "id": "cO16SOIvD9N3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the checkpoint manager.\n",
        "ckpt = tf.train.Checkpoint(model = tax_model, optimizer = optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep = None)"
      ],
      "metadata": {
        "id": "G6q6yWJEsxOI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restore the checkpoint 5 - best performing.\n",
        "#ckpt_manager.latest_checkpoint:\n",
        "ckpt.restore(ckpt_manager.checkpoints[4])\n",
        "print('Checkpoint 5 restored!!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tl0CPi5nGXb0",
        "outputId": "7d50ab2d-2a46-43ca-def8-ac8fd918e4c8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint 5 restored!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test dataset\n",
        "_, test_dataset = create_datasets.create_datasets(tokenizer, data_path, buffer_size, batch_size, max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YI3uRGHv13h",
        "outputId": "27df8d44-6ffc-4cc4-830b-4a66477c1e65"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size is 50265.\n",
            "Reading data from \"/content/EmpatheticIntents/datasets/train_data/train.txt\"...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25023/25023 [00:04<00:00, 6034.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset with 25023 examples.\n",
            "Reading data from \"/content/EmpatheticIntents/datasets/train_data/valid.txt\"...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3544/3544 [00:00<00:00, 6179.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset with 3544 examples.\n",
            "Reading data from \"/content/EmpatheticIntents/datasets/train_data/test.txt\"...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3225/3225 [00:00<00:00, 5513.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset with 3225 examples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate():\n",
        "  accuracy = []\n",
        "  for (batch, inputs) in enumerate(test_dataset):\n",
        "    inp, tar_emot = inputs\n",
        "    enc_padding_mask = utilities.create_masks(inp)\n",
        "    pred_emot = tax_model(inp, False, enc_padding_mask)\n",
        "    pred_emot = np.argmax(pred_emot.numpy(), axis = 1)\n",
        "    accuracy.append(np.mean(tar_emot.numpy() == pred_emot))\n",
        "    #print(f\"tar_emot {tar_emot} pred_emot {pred_emot}\")\n",
        "  return np.mean(accuracy)"
      ],
      "metadata": {
        "id": "13rcZslxu7AU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ac = validate()\n",
        "print('Current accuracy on validation set: {}\\n'.format(val_ac))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4k_KQNC0Yig",
        "outputId": "57a98b9c-8989-4cdc-8f8e-53ee2bf89c2f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current accuracy on validation set: 0.6588242574257426\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**annotation** **code**"
      ],
      "metadata": {
        "id": "0PLMqgMJ9mKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_size = 1\n",
        "SOS_ID = tokenizer.encode('<s>')[0]\n",
        "EOS_ID = tokenizer.encode('</s>')[0]"
      ],
      "metadata": {
        "id": "_7WjQftzOKt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_emotion(utterances):\n",
        "  bs = 1\n",
        "  utterance_IDs = np.ones((len(utterances), max_length), dtype = np.int32)\n",
        "  i = 0\n",
        "  u = utterances[0]\n",
        "  u_IDs = [SOS_ID] + tokenizer.encode(u)[:(max_length-2)] + [EOS_ID]\n",
        "  utterance_IDs[i, :len(u_IDs)] = u_IDs\n",
        "  utterance_emotions = np.zeros((len(utterances), num_emotions))\n",
        "  num_batches = len(utterances) // bs\n",
        "  i = 0\n",
        "  s = i * bs\n",
        "  t = s + bs\n",
        "  inp = tf.constant(utterance_IDs[s:t])\n",
        "  enc_padding_mask = utilities.create_mask(inp)\n",
        "  pred = tax_model(inp, False, enc_padding_mask)\n",
        "  pred = tf.nn.softmax(pred).numpy()\n",
        "  return pred[0]"
      ],
      "metadata": {
        "id": "wgG6eepxZbkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z_cFEDVQMRRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (0, len(emotions)):\n",
        "  emotion = emotions[i]\n",
        "  print(f'Annotating emotion {emotion}')\n",
        "  with open('/content/EmpatheticIntents/datasets/empatheticdialogues_unannotated/'+emotion+'.csv') as infile:\n",
        "    with open('/content/Project_Chatbot/unannotated/'+emotion+'.csv', 'a') as outfile:\n",
        "      writer = csv.writer(outfile, delimiter=str(','), lineterminator='/n')\n",
        "      readCSV = csv.reader(infile, delimiter=',')\n",
        "      writable_row = ['Dialog_ID', 'Type', 'Actor', 'Text', 'Label']\n",
        "      writer.writerow(writable_row)\n",
        "      count = 0\n",
        "      for row in readCSV:\n",
        "        if count >= 1:\n",
        "          writable_row = [row[0], row[1], row[2], row[3]]\n",
        "          text = row[3]\n",
        "          text = text.strip()\n",
        "          predictions = predict_emotion([text])\n",
        "          predictions = np.array(predictions)\n",
        "          indices = predictions.argsort()[-1:][::-1]\n",
        "          writable_row.append(labels[indices[0]])\n",
        "          writer.writerow(writable_row)\n",
        "          count += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "WOTRRUCwOLoU",
        "outputId": "46a4416d-eed7-46ba-d64a-b79e4dd6c663"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annotating emotion afraid\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotADirectoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-c17d2b7ee1c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Annotating emotion {emotion}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/EmpatheticIntents/datasets/empatheticdialogues_unannotated/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Project_Chatbot/unannotated/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m       \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mreadCSV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/content/Project_Chatbot/unannotated/afraid.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HvXE3qviDi4N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}